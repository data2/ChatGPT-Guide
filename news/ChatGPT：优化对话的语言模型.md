我们很高兴推出 ChatGPT 以获取用户的反馈并了解其优势和劣势。在研究预览期间，ChatGPT 的使用是免费的。立即在chat.openai.com上试用。

样品
在以下示例中，ChatGPT提出澄清问题以调试代码。

![image](https://user-images.githubusercontent.com/13504729/216573342-5567d37c-eab5-4c37-92bc-5e4f62e83db0.png)

方法
我们使用与InstructGPT相同的方法，使用来自人类反馈的强化学习 (RLHF) 来训练该模型，但数据收集设置略有不同。我们使用监督微调训练了一个初始模型：人类 AI 训练员提供对话，他们在对话中扮演双方——用户和 AI 助手。我们让培训师可以访问模型编写的建议，以帮助他们撰写回复。我们将这个新的对话数据集与 InstructGPT 数据集混合，我们将其转换为对话格式。

为了创建强化学习的奖励模型，我们需要收集比较数据，其中包含两个或多个按质量排序的模型响应。为了收集这些数据，我们收集了 AI 培训师与聊天机器人的对话。我们随机选择了一条模型编写的消息，抽取了几个备选的完成方式，并让 AI 培训师对它们进行排名。使用这些奖励模型，我们可以使用近端策略优化来微调模型。我们对这个过程进行了几次迭代。
